{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Linear Regression Code"],"metadata":{"id":"LXFIb6hFAwVl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jD4Rr9E_-jTg"},"outputs":[],"source":["import numpy as np\n","import time"]},{"cell_type":"markdown","source":["TODO: Make code nicer"],"metadata":{"id":"cJjOT1QA-2qp"}},{"cell_type":"code","source":["class LinearRegression:\n","    \"\"\"\n","    Simple Linear Regression implementation from scratch\n","    \"\"\"\n","    def __init__(self, lr=0.01, num_iters=1000):\n","        \"\"\"\n","        Initialize the linear regression model\n","\n","        Parameters\n","        ----------\n","        lr : int\n","            learning rate for gradient descen\n","        y : int\n","            number of iterations for gradient descent\n","        \"\"\"\n","        self.lr = lr\n","        self.num_iters = num_iters\n","        self.weights = None\n","        self.bias = None\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Fit the linear regression model to the input data using gradient descent.\n","\n","        Parameters\n","        ----------\n","        X : array-like, shape (n_samples, n_features)\n","            Training data.\n","        y : array-like, shape (n_samples,)\n","            Target values.\n","        \"\"\"\n","        n_samples, n_features = X.shape\n","\n","        # initialize weights and bias\n","        self.weights = np.zeros(n_features)\n","        self.bias = 0\n","\n","        # gradient descent\n","        for i in range(self.num_iters):\n","            # make predictions for current weights and bias\n","            y_predicted = np.dot(X, self.weights) + self.bias\n","\n","            # calculate gradients\n","            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n","            db = (1 / n_samples) * np.sum(y_predicted - y)\n","\n","            # update weights and bias using gradients and learning rate\n","            self.weights -= self.lr * dw\n","            self.bias -= self.lr * db\n","\n","    def fast_fit(self, X, y):\n","        \"\"\"\n","        Fit the linear regression model to the input data using linear algebra.\n","\n","        Parameters\n","        ----------\n","        X : array-like, shape (n_samples, n_features)\n","            Training data.\n","        y : array-like, shape (n_samples,)\n","            Target values.\n","        \"\"\"\n","\n","        # Compute the coefficient and intercept\n","        self.weights = np.linalg.inv(X.T @ X) @ X.T @ y\n","        self.bias = np.mean(y) - np.mean(X, axis=0) @ self.weights\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predict target values for input data.\n","\n","        Parameters\n","        ----------\n","        X : array-like, shape (n_samples, n_features)\n","            Input data.\n","\n","        Returns\n","        -------\n","        y_pred : array-like, shape (n_samples,)\n","            Predicted target values.\n","        \"\"\"\n","        y_predicted = np.dot(X, self.weights) + self.bias\n","        return y_predicted"],"metadata":{"id":"Ndoh3H6x-zn3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create some toy data\n","X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","y = np.array([3, 7, 11])"],"metadata":{"id":"kLmxzDvk-4VR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create an instance of the linear regression model\n"],"metadata":{"id":"VcxnI2q3-8CA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = time.time()\n","# train the model on the data\n","\n","print(f\"{time.time() - start} seconds\")\n","print(lr.weights, lr.bias)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U5ZY0Ve4GC-7","executionInfo":{"status":"ok","timestamp":1679518867712,"user_tz":240,"elapsed":213,"user":{"displayName":"Frankie Willard","userId":"08994884699414581171"}},"outputId":"5f13e2ad-7f63-4b98-96bb-2993600535f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.23621892929077148 seconds\n","[0.33333333 0.44444444 0.55555556] 0.11111111111111449\n"]}]},{"cell_type":"code","source":["# Create another model with a learning rate of 0.01"],"metadata":{"id":"6GIVSMJfIeVc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = time.time()\n","# train the model on the data\n","\n","print(f\"{time.time() - start} seconds\")\n","print(lr2.weights, lr2.bias)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KXUiWrBvGDPP","executionInfo":{"status":"ok","timestamp":1679518874986,"user_tz":240,"elapsed":104,"user":{"displayName":"Frankie Willard","userId":"08994884699414581171"}},"outputId":"4dac69a9-0c20-4712-88cd-faa3a4e31090"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0010671615600585938 seconds\n","[-0.671875  0.        1.265625] 2.09375\n"]}]},{"cell_type":"code","source":["# make predictions on new data\n","X_new = np.array([[10, 11, 12], [13, 14, 15]])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZyfRulL9-8Tc","executionInfo":{"status":"ok","timestamp":1679516499090,"user_tz":240,"elapsed":9,"user":{"displayName":"Frankie Willard","userId":"08994884699414581171"}},"outputId":"8e871184-7589-4364-ece1-4716621d4a79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[15.00030711 19.00049755]\n"]}]},{"cell_type":"markdown","source":["# Linear Regression Math Explanation"],"metadata":{"id":"0QezjjldA0jt"}},{"cell_type":"markdown","source":["Given a linear equation $y_{predicted} = Xw + b$ that models the relationship between the input features $X$ and the target values $y$, we want to find the values of $w$ and $b$ that minimize the mean squared error (MSE) loss function:\n","\n","$$L(w,b) = \\frac{1}{m} \\sum_{i=1}^m (y_{predicted}^{(i)} - y^{(i)})^2$$\n","\n","where $m$ is the number of samples in the dataset."],"metadata":{"id":"MfFB-_xXAUgA"}},{"cell_type":"markdown","source":["To minimize the loss function, we can use gradient descent to iteratively update the values of $w$ and $b$ in the direction of the negative gradient of the loss function with respect to these parameters. The update rule for each iteration of gradient descent is given by:\n","\n","$$w := w - \\alpha \\frac{\\partial L}{\\partial w}$$\n","\n","$$b := b - \\alpha \\frac{\\partial L}{\\partial b}$$\n","\n","where $\\alpha$ is the learning rate.\n","\n"],"metadata":{"id":"yCWqFL3lAc15"}},{"cell_type":"markdown","source":["To compute the partial derivatives, we can use the chain rule:\n","\n","$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y_{predicted}} \\frac{\\partial y_{predicted}}{\\partial w}$$\n","\n","$$\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y_{predicted}} \\frac{\\partial y_{predicted}}{\\partial b}$$\n","\n","where\n","\n","$$\\frac{\\partial L}{\\partial y_{predicted}} = \\frac{2}{m} (y_{predicted} - y)$$\n","\n","is the derivative of the MSE loss function with respect to $y_{predicted}$."],"metadata":{"id":"3wfoRL40AgeT"}},{"cell_type":"markdown","source":["To compute the derivatives of $y_{predicted}$ with respect to $w$ and $b$, we can take the partial derivatives of the linear equation:\n","\n","$$\\frac{\\partial y_{predicted}}{\\partial w} = X$$\n","\n","$$\\frac{\\partial y_{predicted}}{\\partial b} = 1$$\n","\n"],"metadata":{"id":"aSelPDS7AoDc"}},{"cell_type":"markdown","source":["Substituting these expressions into the chain rule gives us:\n","\n","$$\\frac{\\partial L}{\\partial w} = \\frac{2}{m} X^T (Xw + b - y)$$\n","\n","$$\\frac{\\partial L}{\\partial b} = \\frac{2}{m} \\sum_{i=1}^m (y_{predicted}^{(i)} - y^{(i)})$$"],"metadata":{"id":"cJZxDmyVA5RH"}},{"cell_type":"markdown","source":["Simplifying these expressions gives us the final formulas for dw and db used in the code:\n","\n","$$dw = \\frac{1}{m} X^T(y_{predicted} - y)$$\n","\n","$$db = \\frac{1}{m} \\sum_{i=1}^m (y_{predicted}^{(i)} - y^{(i)})$$\n","\n","where $m$ is the number of samples, $X$ is the matrix of input features, $y$ is the vector of target values, and $y_{predicted}$ is the vector of predicted values given by the linear equation $y_{predicted} = Xw + b$."],"metadata":{"id":"plyHUqK2A8i4"}}]}