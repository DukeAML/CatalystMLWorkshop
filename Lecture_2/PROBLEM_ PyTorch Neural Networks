{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qUm1MqDD-T6J"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","import numpy as np"]},{"cell_type":"markdown","source":["TODO: \n","* Add weight decay, L1/L2 norm??\n","* Use real dataset?\n","* Categorical features?\n","* Basic CUDA?\n","* Look into num parameters/weight distribution?\n","* Weight initialization\n","\n","* https://github.com/frankwillard/Graduate-Deep-Neural-Nets-Homework/blob/main/HW1/SimpleNN.ipynb"],"metadata":{"id":"Kt-YY5_PGwVG"}},{"cell_type":"code","source":["# Define the neural network class\n","class NeuralNetwork(nn.Module):\n","    \"\"\"\n","    Basic PyTorch neural network for regression.\n","    \"\"\"\n","    def __init__(self, input_size, hidden_size, output_size):\n","        \"\"\"\n","        Constructor for the Net class.\n","\n","        Parameters\n","        ----------\n","        input_size : int\n","            Number of input features.\n","        hidden_size : int\n","            Number of hidden units.\n","        output_size : int\n","            Number of output features.\n","        \"\"\"\n","        super(NeuralNetwork, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size, output_size)\n","        \n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the neural network.\n","\n","        Parameters\n","        ----------\n","        x : tensor, shape (n_samples, input_size)\n","            Input tensor.\n","\n","        Returns\n","        -------\n","        tensor, shape (n_samples, output_size)\n","            Output tensor.\n","        \"\"\"\n","        out = self.fc1(x)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        return out"],"metadata":{"id":"BaPFuyAK-aHL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here is a brief explanation of the components of this neural network above:"],"metadata":{"id":"upPJThnoAzoc"}},{"cell_type":"markdown","source":["`NeuralNetwork`: This is a class that inherits from the nn.Module class in PyTorch, which is used to define neural network models. The class takes in three parameters: `input_size`, `hidden_size`, and `output_size`, which represent the number of input features, the number of hidden units, and the number of output features, respectively."],"metadata":{"id":"DoNt5SqFA5hw"}},{"cell_type":"markdown","source":["`nn.Linear`: This is a module in PyTorch that represents a linear transformation of the input data. In this code, it is used to define two fully connected layers (`fc1` and `fc2`) that transform the input data from the input size to the hidden size and from the hidden size to the output size, respectively.\n","\n","In a neural network, a linear transformation can be thought of as a matrix multiplication followed by an addition of a bias term. Specifically, given an input tensor x of shape (`batch_size`, `input_size`) and a linear layer `nn.Linear(input_size, output_size)`, the output tensor of the linear layer is \n","\n","```\n","out = x @ W^T + b\n","```\n","\n","where `W` is a weight matrix of shape (`input_size`, `output_size`), `b` is a bias term of shape (`output_size`,), and `@` represents matrix multiplication.\n","\n","The weight matrix `W` and bias term `b` are learned during the training process to minimize the difference between the predicted and actual outputs. In other words, the neural network learns the optimal values of W and b to map the input features to the desired output features.\n","\n","`nn.Linear` is often used to define fully connected layers in a neural network. A fully connected layer is a layer in which each neuron is connected to every neuron in the previous layer. \n"],"metadata":{"id":"NKz5u1e8A5kD"}},{"cell_type":"markdown","source":["`nn.ReLU`: This is an activation function in PyTorch that applies a rectified linear unit (ReLU) function to the output of the linear layer. The ReLU function returns the input value if it is positive, and returns 0 if it is negative."],"metadata":{"id":"CU0t7f-oA5nn"}},{"cell_type":"markdown","source":["`forward`: This is a method in PyTorch that defines the forward pass of the neural network. It takes in an input tensor `x`, which has a shape of (`n_samples`, `input_size`), and passes it through the two fully connected layers (`fc1` and `fc2`) and the ReLU activation function (`relu`). The output of the final fully connected layer is returned as the output of the neural network, with a shape of (`n_samples`, `output_size`)."],"metadata":{"id":"vfesGNJqA5pq"}},{"cell_type":"markdown","source":["In summary, this code defines a neural network that takes in an input tensor, passes it through two linear layers and a ReLU activation function, and returns an output tensor. The neural network can be trained on a dataset with input and output features of the specified sizes using a suitable optimization algorithm to minimize the difference between the predicted and actual outputs."],"metadata":{"id":"X_yXIZEjBSDz"}},{"cell_type":"code","source":["# Define the hyperparameters\n","learning_rate = 0.01\n","momentum = 0.9\n","num_epochs = 1000\n","batch_size = 16"],"metadata":{"id":"hI8w4Q4D_QSg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the context of a neural network, hyperparameters are parameters that are set before training begins and are not learned during training. These parameters can have a significant impact on the performance of the neural network. Here is a brief explanation of the hyperparameters being defined:\n"],"metadata":{"id":"_xhxPQlBACT2"}},{"cell_type":"markdown","source":["`learning_rate`: This is a hyperparameter that controls how much the weights of the neural network are adjusted during training. It determines the step size at each iteration while moving toward a minimum of a loss function. A smaller learning rate generally leads to slower but more precise convergence, while a larger learning rate can lead to faster convergence but may result in overshooting the minimum of the loss function."],"metadata":{"id":"arjDeVBmADLi"}},{"cell_type":"markdown","source":["`momentum`: This is a hyperparameter used in the optimizer that influences the update of the model's parameters during training. It controls the amount of influence that the previous updates have on the current update. Momentum helps to accelerate the convergence towards the optimum solution and smooth out the variations in the gradient updates. This is because it reduces the effect of short-term fluctuations in the gradient and amplifies the effect of long-term trends.\n","\n","The value of 0.9 is a common default value and has been shown to work well in practice for many types of deep learning models. The 0.9 means that the contribution of the previous update is 90% and the contribution of the current update is 10%."],"metadata":{"id":"AgUHnUFNGJMw"}},{"cell_type":"markdown","source":["`num_epochs`: This is a hyperparameter that specifies the number of times the entire dataset will be iterated over during training. An epoch is defined as one complete pass through the entire dataset. Increasing the number of epochs can lead to better performance of the model up to a certain point, after which it may start to overfit the training data. \n","\n","Early stopping is a regularization technique used in machine learning to prevent overfitting by stopping the training process before the model has fully converged (before it has trained on `num_epochs`). It does this by monitoring the performance of the model on a validation set during training and stopping training when the performance on the validation set stops improving."],"metadata":{"id":"n5MshN2TAHns"}},{"cell_type":"markdown","source":["`batch_size`: This is a hyperparameter that specifies the number of training examples used in one iteration of gradient descent. A larger batch size can lead to faster convergence as more examples are processed in each iteration, but can also lead to higher memory requirements and longer training times per epoch. A smaller batch size can lead to slower convergence but may result in a more stable and accurate model."],"metadata":{"id":"kSp9KWq3AJ-6"}},{"cell_type":"code","source":["# Define the input, hidden, and output sizes\n","input_size = 1\n","hidden_size = 10\n","output_size = 1"],"metadata":{"id":"wen834WU_QUx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the neural network\n"],"metadata":{"id":"InBPnGV8_Sa7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the loss function using Mean Squared Loss\n"],"metadata":{"id":"6t2OCPQ8_SdY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this case, the mean squared error (MSE) loss function is used. MSE measures the average squared difference between the predicted and actual output values. In other words, it computes the average of the squared differences between the predicted and actual output values for all the training samples. The goal of training the neural network is to minimize this loss function."],"metadata":{"id":"skNSFEH8CK0f"}},{"cell_type":"markdown","source":["Not important for this problem, but if we were doing classification, however, we would likely use the cross-entropy loss.\n","\n","The Binary Cross Entropy (BCE) loss function is commonly used in binary classification problems where the goal is to predict a binary label (0 or 1) for each input example. Given a binary classification problem, the BCE loss function measures the difference between the predicted probabilities and the true labels. Specifically, it computes the average of the binary cross-entropy loss over all the training examples. The binary cross-entropy loss for a single training example is given by:\n","```\n","loss(x, y) = - (y * log(x) + (1 - y) * log(1 - x))\n","```\n","\n","Here, `x` is the predicted probability for the positive class (i.e., the probability that the label is 1), `y` is the true label (0 or 1), and `log` is the natural logarithm function."],"metadata":{"id":"Zee60WAwDM5I"}},{"cell_type":"markdown","source":["In PyTorch, the BCE loss function is implemented as `torch.nn.BCELoss()`. The BCE loss function can be used as follows:\n","\n","```\n","criterion = nn.BCELoss()\n","...\n","loss = criterion(outputs, labels)\n","```\n","\n","Here, `outputs` are the predicted probabilities (output of the last layer of the neural network) and `labels` are the true binary labels."],"metadata":{"id":"zEV5X8zeEGpU"}},{"cell_type":"code","source":["# Define the optimizer (use Stochastic Gradient Descent)\n"],"metadata":{"id":"M3Zu6gMDDHqt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this case, stochastic gradient descent (SGD) is used. SGD is a widely used optimization algorithm for training neural networks. It works by computing the gradient of the loss function with respect to the weights of the neural network and using this gradient to update the weights. The learning rate `lr` is a hyperparameter that controls the step size of the weight updates. A higher learning rate can lead to faster convergence but may also result in overshooting the optimal weights, while a lower learning rate may converge more slowly but may be more precise in finding the optimal weights."],"metadata":{"id":"q2F852fWCbgL"}},{"cell_type":"markdown","source":["Not important for this problem, but there are other common optimizers used in deep learning that may be better for other problems you work on.\n","\n"],"metadata":{"id":"3X1Gcu3tDvXV"}},{"cell_type":"markdown","source":["To use the Adam optimizer in PyTorch, you can use the torch.optim.Adam class. Here's an example of how to use it:\n","\n","```\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","```\n","\n","In this example, `model` is the neural network model you are training, and `lr` is the learning rate."],"metadata":{"id":"D7q035zQEJWb"}},{"cell_type":"markdown","source":["To use the RMSProp optimizer in PyTorch, you can use the torch.optim.Adam class. Here's an example of how to use it:\n","\n","```\n","optimizer = optim.RMSProp(model.parameters(), lr=0.001)\n","\n","```\n","\n","Like with the Adam optimizer, `model` is the neural network model you are training, and `lr` is the learning rate."],"metadata":{"id":"sDLzqSEbEQWI"}},{"cell_type":"markdown","source":["As for the advantages and disadvantages of Adam, RMSProp, and SGD:\n","\n","*   SGD is the simplest optimizer and can work well for simple models and small datasets. However, it can be slow to converge and can get stuck in local minima.\n","*   RMSProp is a modification of SGD that adapts the learning rate for each parameter based on the history of the gradients. This can help it converge faster than SGD. However, it can still get stuck in local minima and can be sensitive to the choice of hyperparameters.\n","*   Adam is a more advanced optimizer that combines ideas from RMSProp and momentum. It can converge faster than RMSProp and can be less sensitive to the choice of hyperparameters. However, it can sometimes converge to suboptimal solutions and can be computationally more expensive than SGD and RMSProp."],"metadata":{"id":"znUqvckjEYj0"}},{"cell_type":"markdown","source":["An adaptive learning rate can be helpful for faster convergence because it adjusts the learning rate during training based on the history of the gradients. This allows the optimizer to take larger steps in parameter space when progress is fast (the beginning of training) and smaller steps when progress is slow (the end of training), which can lead to faster convergence.\n","\n","Overall, the choice of optimizer depends on the specific problem you are trying to solve and the properties of your dataset and model. It's often a good idea to try out different optimizers and see which one works best for your problem."],"metadata":{"id":"MleKe5d_E_gk"}},{"cell_type":"code","source":["# Generate some example data\n","X = torch.randn(100, input_size)\n","y = 2*X + torch.randn(100, output_size)*0.1"],"metadata":{"id":"iOYU4lCF_Sgj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a DataLoader for the data\n"],"metadata":{"id":"DWFl8Exm_Shg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`dataloader`: An iterator that provides batches of the training data. Batching the data reduces the memory usage and allows the network to learn more efficiently.\n"],"metadata":{"id":"l4H15bFrFfLZ"}},{"cell_type":"code","source":["# Train the neural network\n","for epoch in range(num_epochs):\n","    for batch in dataloader:\n","        # Zero the gradients\n","        \n","\n","        # Forward pass\n","        \n","\n","        # Compute the loss\n","        \n","\n","        # Backward pass and optimization\n","        \n","\n","    # Print the loss every 100 epochs\n","    "],"metadata":{"id":"KeRjMJWX_dKz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`optimizer.zero_grad()`: Before computing the gradients, we need to clear the gradients of all optimized variables. The gradients of the previous batch can interfere with the gradients computed on the current batch, and cause gradient descent to take longer to converge."],"metadata":{"id":"-BG5zulbCMZP"}},{"cell_type":"markdown","source":["`model(inputs)`: This calculates the forward pass of the neural network using the input data inputs to produce the predicted output outputs.\n"],"metadata":{"id":"AiESKihDFjxm"}},{"cell_type":"markdown","source":["`criterion`: The loss function used to calculate the difference between the predicted output and the actual target output. In this code, the loss function is mean squared error (nn.MSELoss())."],"metadata":{"id":"z_06WLoPFj0w"}},{"cell_type":"markdown","source":["`loss.backward()`: This computes the gradients of the loss with respect to the parameters of the neural network using backpropagation."],"metadata":{"id":"tj-QLTQIFj5R"}},{"cell_type":"markdown","source":["`optimizer.step()`: This updates the parameters of the neural network using the computed gradients and the optimizer algorithm.\n"],"metadata":{"id":"0AHAQsdKFrjs"}},{"cell_type":"code","source":["# evaluate the neural network on the test data\n"],"metadata":{"id":"cMVQZPgl_qT-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"uV629hyoCM6d"}}]}