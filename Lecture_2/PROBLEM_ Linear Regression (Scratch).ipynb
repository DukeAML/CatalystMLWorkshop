{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXFIb6hFAwVl"
   },
   "source": [
    "# Linear Regression Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jD4Rr9E_-jTg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ndoh3H6x-zn3"
   },
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Simple Linear Regression implementation from scratch\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01, num_iters=1000):\n",
    "        \"\"\"\n",
    "        Initialize the linear regression model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr : int\n",
    "            learning rate for gradient descen\n",
    "        y : int\n",
    "            number of iterations for gradient descent\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.num_iters = num_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the linear regression model to the input data using gradient descent.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # initialize weights and bias\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # gradient descent\n",
    "        for i in range(self.num_iters):\n",
    "            # make predictions for current weights and bias\n",
    "            y_predicted = np.dot(X, self.weights) + self.bias\n",
    "\n",
    "            # calculate gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # update weights and bias using gradients and learning rate\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for input data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array-like, shape (n_samples,)\n",
    "            Predicted target values.\n",
    "        \"\"\"\n",
    "        y_predicted = np.dot(X, self.weights) + self.bias\n",
    "        return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLmxzDvk-4VR"
   },
   "outputs": [],
   "source": [
    "# create some toy data\n",
    "X, y, coef = make_regression(n_samples=100,#number of samples\n",
    "                                      n_features=5,#number of features\n",
    "                                      n_informative=3,#number of useful features \n",
    "                                      noise=10,#bias and standard deviation of the guassian noise\n",
    "                                      coef=True,#true coefficient used to generated the data\n",
    "                                      random_state=0) #set for same data points for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcxnI2q3-8CA"
   },
   "outputs": [],
   "source": [
    "# create an instance of the linear regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1679518867712,
     "user": {
      "displayName": "Frankie Willard",
      "userId": "08994884699414581171"
     },
     "user_tz": 240
    },
    "id": "U5ZY0Ve4GC-7",
    "outputId": "5f13e2ad-7f63-4b98-96bb-2993600535f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23621892929077148 seconds\n",
      "[0.33333333 0.44444444 0.55555556] 0.11111111111111449\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# train the model on the data\n",
    "\n",
    "print(f\"{time.time() - start} seconds\")\n",
    "print(lr.weights, lr.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1679516499090,
     "user": {
      "displayName": "Frankie Willard",
      "userId": "08994884699414581171"
     },
     "user_tz": 240
    },
    "id": "ZyfRulL9-8Tc",
    "outputId": "8e871184-7589-4364-ece1-4716621d4a79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.00030711 19.00049755]\n"
     ]
    }
   ],
   "source": [
    "# make predictions on new data\n",
    "X_new = np.array([[10, 11, 12, 13, 14]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as SkLinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GIVSMJfIeVc"
   },
   "outputs": [],
   "source": [
    "# Create another model with the sklearn linear regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1679518874986,
     "user": {
      "displayName": "Frankie Willard",
      "userId": "08994884699414581171"
     },
     "user_tz": 240
    },
    "id": "KXUiWrBvGDPP",
    "outputId": "4dac69a9-0c20-4712-88cd-faa3a4e31090"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010671615600585938 seconds\n",
      "[-0.671875  0.        1.265625] 2.09375\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# train the model on the data\n",
    "\n",
    "print(f\"{time.time() - start} seconds\")\n",
    "print(lr2.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QezjjldA0jt"
   },
   "source": [
    "# Linear Regression Math Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfFB-_xXAUgA"
   },
   "source": [
    "Given a linear equation $y_{predicted} = Xw + b$ that models the relationship between the input features $X$ and the target values $y$, we want to find the values of $w$ and $b$ that minimize the mean squared error (MSE) loss function:\n",
    "\n",
    "$$L(w,b) = \\frac{1}{m} \\sum_{i=1}^m (y_{predicted}^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "where $m$ is the number of samples in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCWqFL3lAc15"
   },
   "source": [
    "To minimize the loss function, we can use gradient descent to iteratively update the values of $w$ and $b$ in the direction of the negative gradient of the loss function with respect to these parameters. The update rule for each iteration of gradient descent is given by:\n",
    "\n",
    "$$w := w - \\alpha \\frac{\\partial L}{\\partial w}$$\n",
    "\n",
    "$$b := b - \\alpha \\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wfoRL40AgeT"
   },
   "source": [
    "To compute the partial derivatives, we can use the chain rule:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y_{predicted}} \\frac{\\partial y_{predicted}}{\\partial w}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y_{predicted}} \\frac{\\partial y_{predicted}}{\\partial b}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial y_{predicted}} = \\frac{2}{m} (y_{predicted} - y)$$\n",
    "\n",
    "is the derivative of the MSE loss function with respect to $y_{predicted}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSelPDS7AoDc"
   },
   "source": [
    "To compute the derivatives of $y_{predicted}$ with respect to $w$ and $b$, we can take the partial derivatives of the linear equation:\n",
    "\n",
    "$$\\frac{\\partial y_{predicted}}{\\partial w} = X$$\n",
    "\n",
    "$$\\frac{\\partial y_{predicted}}{\\partial b} = 1$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJZxDmyVA5RH"
   },
   "source": [
    "Substituting these expressions into the chain rule gives us:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{2}{m} X^T (Xw + b - y)$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{2}{m} \\sum_{i=1}^m (y_{predicted}^{(i)} - y^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plyHUqK2A8i4"
   },
   "source": [
    "Simplifying these expressions gives us the final formulas for dw and db used in the code:\n",
    "\n",
    "$$dw = \\frac{1}{m} X^T(y_{predicted} - y)$$\n",
    "\n",
    "$$db = \\frac{1}{m} \\sum_{i=1}^m (y_{predicted}^{(i)} - y^{(i)})$$\n",
    "\n",
    "where $m$ is the number of samples, $X$ is the matrix of input features, $y$ is the vector of target values, and $y_{predicted}$ is the vector of predicted values given by the linear equation $y_{predicted} = Xw + b$."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
